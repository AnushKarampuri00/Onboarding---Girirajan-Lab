# CLUSTER 

## About the Cluster (HPC Basics)

* For those unfamiliar with HPC systems: A cluster is essentially a large-scale computer system with hundreds to thousands of cores (CPUs), GPUs, and terabytes of memory and storage.
* Each cluster node is a powerful computer with multiple CPUs and large RAM, used for computation.
* The Girirajan Lab has a dedicated cluster with five computation nodes:

| Node Name | Cores | Special Features |
|-----------|-------|------------------|
| RAMONA    | 40    | General computation |
| DURGA     | 128   | High-memory tasks |
| QINGYU    | 160   | Large-scale jobs |
| SARAH     | 192   | High-throughput tasks |
| LAILA     | 112   | Includes GPU support for deep learning |

Additionally, the cluster also includes eight storage nodes: data, …... data7.

* Note: : Penn State provides a separate HPC resource called ROAR (Routinely Available Online Resources), which can be rented for larger jobs.


## User Profile & Storage

* Each user receives 1–2 TB of personal storage space (home directory).
* Shared datasets are stored on the common storage nodes (data to data7).